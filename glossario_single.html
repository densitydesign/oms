<!DOCTYPE html>
<html>
<head>
	<title>WHO</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Bootstrap -->
	<link href="css/bootstrap.min.css" rel="stylesheet">
	<link href="css/app.css" rel="stylesheet">
	<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>


	<script src="https://code.jquery.com/jquery.js"></script>
	<script src="js/bootstrap.min.js"></script>
	<script src="js/jquery.mousewheel.js"></script>
	<script src="js/d3.v3.min.js"></script>
	<script src="js/hammer.min.js"></script>
	<script src="js/jquery.hammer.min.js"></script>
	<script type="text/javascript" src="js/denseShow.js"></script>


	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
      <![endif]-->
  </head>
  <body>
  	<div class="container">	
  		<div class="glossary">
  			<div class="row section-row">
  				<div class="col-md-6 col-md-offset-3 section-definition">
  					<p class="section-text"><span class="glossary-name">crawl&middot;ing &mdash;</span> Web crawler starts with a list of URLs to visit, called the seeds. As the crawler visits these URLs, it identifies all the hyperlinks in the page and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies.
  					The large volume implies that the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change implies that the pages might have already been updated or even deleted.</p>
  					</div>
  				</div>
  			</div>
  		</div>


  	</body>
  	</html>